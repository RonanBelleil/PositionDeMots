{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annoying review at 692 with length 1104\n",
      "annoying review at 1117 with length 888\n",
      "annoying review at 1238 with length 660\n",
      "Camembert loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.model_selection\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MonDataset(Dataset):\n",
    "    \"Création d'un dataset personnalisé avec PyTorch\"\n",
    "\n",
    "    def __init__(self, phrases, labels, transformations=None):\n",
    "        \"Initialisation\"\n",
    "\n",
    "        self.phrases = phrases\n",
    "        self.labels = labels\n",
    "        self.len = len(phrases)\n",
    "\n",
    "        self.transform = transformations # Si jamais tu veux faire des transformations sur tes phrases ou tes labels, tu stock la fonction ici\n",
    "\n",
    "    def __len__(self):\n",
    "        'Retourne le nombre de items dans ton dataset'\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"Fonction qui permet de récupérer un item de ton dataset, ex: une phrase\"\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.phrases[i]\n",
    "        y = self.labels[i]\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X) # par exemple, lemmatiser la phrase ou autre\n",
    "\n",
    "        return X, y\n",
    "\n",
    "df = pd.read_csv(\"POS1.csv\", names = ['message','label'], header=1)\n",
    "df.fillna('',inplace=True)\n",
    "#df.drop([692,1238],inplace=True)\n",
    "max_len = 0\n",
    "for i,sent in enumerate(df[\"message\"]):\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    if len(input_ids) > 512:\n",
    "        print(\"annoying review at\", i,\"with length\",\n",
    "              len(input_ids))\n",
    "        df.drop([i],inplace=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "\n",
    "tokenizer = transformers.CamembertTokenizer.from_pretrained(\"camembert/camembert-large\")\n",
    "model = transformers.CamembertModel.from_pretrained(\"camembert/camembert-large\")\n",
    "\n",
    "dataset = MonDataset(df['message'].to_list(), df['label'].to_list())\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Camembert loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch process\n",
      "0\n",
      "16\n",
      "32\n",
      "48\n",
      "64\n",
      "80\n",
      "96\n",
      "112\n",
      "128\n",
      "144\n",
      "160\n",
      "176\n",
      "192\n",
      "208\n",
      "224\n",
      "240\n",
      "256\n",
      "272\n",
      "288\n",
      "304\n",
      "320\n",
      "336\n",
      "352\n",
      "368\n",
      "384\n",
      "400\n",
      "416\n",
      "432\n",
      "448\n",
      "464\n",
      "480\n",
      "496\n",
      "512\n",
      "528\n",
      "544\n",
      "560\n",
      "576\n",
      "592\n",
      "608\n",
      "624\n",
      "640\n",
      "656\n",
      "672\n",
      "688\n",
      "704\n",
      "720\n",
      "736\n",
      "752\n",
      "768\n",
      "784\n",
      "800\n",
      "816\n",
      "832\n",
      "848\n",
      "864\n",
      "880\n",
      "896\n",
      "912\n",
      "928\n",
      "944\n",
      "960\n",
      "976\n",
      "992\n",
      "1008\n",
      "1024\n",
      "1040\n",
      "1056\n",
      "1072\n",
      "1088\n",
      "1104\n",
      "1120\n",
      "1136\n",
      "1152\n",
      "1168\n",
      "1184\n",
      "1200\n",
      "1216\n",
      "1232\n",
      "1248\n",
      "1264\n",
      "1280\n",
      "1296\n",
      "1312\n"
     ]
    }
   ],
   "source": [
    "print(\"batch process\")\n",
    "outputNumpy = []\n",
    "yBatch = []\n",
    "i=0\n",
    "\n",
    "for batch in dataloader:\n",
    "    tokens_ids = [tokenizer.encode(sentence) for sentence in batch[0]]\n",
    "    tokens_ids_unsq = [torch.tensor(token_id).unsqueeze(0) for token_id in tokens_ids]\n",
    "    outputs = [model(token)[\"last_hidden_state\"][:,0] for token in tokens_ids_unsq]\n",
    "    outputNumpy += [output.squeeze(0).detach().numpy() for output in outputs]\n",
    "    yBatch += batch[1]\n",
    "    print(i)\n",
    "    i += 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(outputNumpy)\n",
    "y = yBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score de la validation croisée :  [0.85869565 0.88619855 0.83673469 0.859375   0.84634761 0.85\n",
      " 0.82849604 0.83412322 0.85514019 0.80965147]\n",
      "Moyenne des F-score 0.8464762426731749\n"
     ]
    }
   ],
   "source": [
    "cv = sklearn.model_selection.ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "clf = svm.SVC(kernel='poly', C=1, random_state=42)\n",
    "results = cross_val_score(clf, X, y, cv=cv, scoring=\"f1\")\n",
    "print(\"F-score de la validation croisée : \", results)\n",
    "print(\"Moyenne des F-score\", np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
